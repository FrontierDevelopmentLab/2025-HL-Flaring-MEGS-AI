{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-25T13:52:03.807412Z",
     "start_time": "2025-07-25T13:52:03.438662Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "state = torch.load(\"/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/flaring/forecasting/training/MEGS-AI ViT Testing Griffin/hztj5skd/checkpoints/epoch=87-step=99000.ckpt\", weights_only=False)\n",
    "model = state['model']\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:54:17.180844Z",
     "start_time": "2025-07-25T13:54:11.159971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "norm = np.load(\"/mnt/data/ML-Ready/mixed_data/SXR/normalized_sxr.npy\")\n",
    "\n",
    "def unnormalize_sxr(normalized_values, sxr_norm):\n",
    "    return 10 ** (normalized_values * float(sxr_norm[1].item()) + float(sxr_norm[0].item())) - 1e-8\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model_kwargs):\n",
    "        super().__init__()\n",
    "        self.lr = model_kwargs['lr']\n",
    "        self.save_hyperparameters()\n",
    "        filtered_kwargs = dict(model_kwargs)\n",
    "        filtered_kwargs.pop('lr', None)\n",
    "        self.model = VisionTransformer(**filtered_kwargs)\n",
    "\n",
    "    def forward(self, x, return_attention=True):\n",
    "        return self.model(x, return_attention=return_attention)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Use AdamW with weight decay for better regularization\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=0.01,  # Add weight decay\n",
    "            betas=(0.9, 0.95)  # Better betas for ViT\n",
    "        )\n",
    "\n",
    "        # Option 1: ReduceLROnPlateau (your current approach, fixed)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #     optimizer,\n",
    "        #     mode='min',\n",
    "        #     factor=0.5,\n",
    "        #     patience=5,  # Increased patience\n",
    "        #     min_lr=1e-7,  # Set minimum LR\n",
    "        # )\n",
    "\n",
    "        # return {\n",
    "        #     'optimizer': optimizer,\n",
    "        #     'lr_scheduler': {\n",
    "        #         'scheduler': scheduler,\n",
    "        #         'monitor': 'val_loss',\n",
    "        #         'interval': 'epoch',\n",
    "        #         'frequency': 1,\n",
    "        #         'strict': True,\n",
    "        #         'name': 'learning_rate'  # This helps with logging\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "        # Option 2: Cosine Annealing with Warmup (recommended)\n",
    "        # Uncomment this section and comment out the above return statement to use\n",
    "\n",
    "\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=10,  # Restart every 10 epochs\n",
    "            T_mult=2,  # Double the cycle length after each restart\n",
    "            eta_min=1e-7  # Minimum learning rate\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1,\n",
    "                'name': 'learning_rate'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # M/X Class Flare Detection Optimized Weights\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        imgs, sxr = batch  # sxr is normalized\n",
    "        preds = self.model(imgs)  # preds are also normalized (model trained on normalized data)\n",
    "        preds_squeezed = torch.squeeze(preds)\n",
    "\n",
    "        # Calculate base loss (on normalized values)\n",
    "        base_loss = F.huber_loss(preds_squeezed, sxr, delta=1.0, reduction='none')\n",
    "\n",
    "        # ===== UNNORMALIZE FOR THRESHOLD COMPARISONS =====\n",
    "        sxr_un = unnormalize_sxr(sxr,norm)  # Convert to actual flux values\n",
    "\n",
    "        preds_squeezed_un = unnormalize_sxr(preds_squeezed,norm)  # Convert predictions too\n",
    "\n",
    "        # ===== M/X CLASS FLARE SPECIFIC THRESHOLDS =====\n",
    "        # GOES SXR flux thresholds for flare classes (in original units):\n",
    "        c_class_threshold = 1e-6  # C-class flares\n",
    "        m_class_threshold = 1e-5  # M-class flares\n",
    "        x_class_threshold = 1e-4  # X-class flares\n",
    "\n",
    "        # ===== STRATIFIED FLUX WEIGHTING =====\n",
    "        # Much more aggressive weighting for M/X class events\n",
    "        flux_weights = torch.ones_like(sxr_un)\n",
    "        flux_weights = torch.where(sxr_un >= c_class_threshold, 2.0, flux_weights)  # C-class: 2x\n",
    "        flux_weights = torch.where(sxr_un >= m_class_threshold, 10.0, flux_weights)  # M-class: 10x\n",
    "        flux_weights = torch.where(sxr_un >= x_class_threshold, 10.0, flux_weights)  # X-class: 25x\n",
    "\n",
    "        # ===== PREDICTION-BASED WEIGHTING FOR M/X =====\n",
    "        # Heavy penalty when model fails to predict M/X class flares\n",
    "        pred_weights = torch.ones_like(preds_squeezed_un)\n",
    "\n",
    "        # If actual is M/X but prediction is too low → massive penalty\n",
    "        missed_m_flares = (sxr_un >= m_class_threshold) & (preds_squeezed_un < m_class_threshold * 0.5)\n",
    "        missed_x_flares = (sxr_un >= x_class_threshold) & (preds_squeezed_un < x_class_threshold * 0.5)\n",
    "\n",
    "        pred_weights = torch.where(missed_m_flares, 10.0, pred_weights)  # 15x penalty for missed M\n",
    "        pred_weights = torch.where(missed_x_flares, 10.0, pred_weights)  # 50x penalty for missed X\n",
    "\n",
    "        # If prediction is M/X class → moderate penalty to reduce false positives\n",
    "        pred_weights = torch.where(preds_squeezed_un >= m_class_threshold, 3.0, pred_weights)\n",
    "\n",
    "        # ===== FALSE ALARM REDUCTION =====\n",
    "        # Penalize false alarms (predicting M/X when actual is lower)\n",
    "        false_m_alarms = (preds_squeezed_un >= m_class_threshold) & (sxr_un < c_class_threshold)\n",
    "        false_x_alarms = (preds_squeezed_un >= x_class_threshold) & (sxr_un < m_class_threshold)\n",
    "\n",
    "        false_alarm_weights = torch.ones_like(sxr_un)\n",
    "        false_alarm_weights = torch.where(false_m_alarms, 5.0, false_alarm_weights)  # 5x penalty for false M\n",
    "        false_alarm_weights = torch.where(false_x_alarms, 5.0, false_alarm_weights)  # 8x penalty for false X\n",
    "\n",
    "        # ===== TEMPORAL CONTEXT WEIGHTING =====\n",
    "        # Weight the hours leading up to M/X flares more heavily\n",
    "        temporal_weights = torch.ones_like(sxr_un)\n",
    "\n",
    "        # ===== COMBINE ALL WEIGHTS =====\n",
    "        total_weights = flux_weights * pred_weights * false_alarm_weights * temporal_weights\n",
    "\n",
    "        # Cap maximum weight to prevent training instability\n",
    "        total_weights = torch.clamp(total_weights, min=1.0, max=100.0)\n",
    "\n",
    "        # Apply weights to loss\n",
    "        weighted_loss = base_loss * total_weights\n",
    "        loss = weighted_loss.mean()\n",
    "\n",
    "        # ===== M/X CLASS SPECIFIC METRICS =====\n",
    "        # Track performance specifically on M/X class events (using unnormalized values)\n",
    "        m_class_mask = sxr_un >= m_class_threshold\n",
    "        x_class_mask = sxr_un >= x_class_threshold\n",
    "\n",
    "        if m_class_mask.any():\n",
    "            m_class_mae = F.l1_loss(preds_squeezed[m_class_mask], sxr[m_class_mask])\n",
    "            self.log(f\"{mode}_m_class_mae\", m_class_mae, sync_dist=True)\n",
    "\n",
    "            # M-class detection rate (did we predict >= M when actual >= M?)\n",
    "            # FIXED: Use unnormalized predictions for threshold comparison\n",
    "            m_detection_rate = (preds_squeezed_un[m_class_mask] >= m_class_threshold).float().mean()\n",
    "            self.log(f\"{mode}_m_detection_rate\", m_detection_rate, sync_dist=True)\n",
    "\n",
    "        if x_class_mask.any():\n",
    "            x_class_mae = F.l1_loss(preds_squeezed[x_class_mask], sxr[x_class_mask])\n",
    "            self.log(f\"{mode}_x_class_mae\", x_class_mae, sync_dist=True)\n",
    "\n",
    "            # X-class detection rate\n",
    "            # FIXED: Use unnormalized predictions for threshold comparison\n",
    "            x_detection_rate = (preds_squeezed_un[x_class_mask] >= x_class_threshold).float().mean()\n",
    "            self.log(f\"{mode}_x_detection_rate\", x_detection_rate, sync_dist=True)\n",
    "\n",
    "        # False alarm rates\n",
    "        # FIXED: Use unnormalized values for threshold comparisons\n",
    "        quiet_mask = sxr_un < c_class_threshold\n",
    "        if quiet_mask.any():\n",
    "            false_m_rate = (preds_squeezed_un[quiet_mask] >= m_class_threshold).float().mean()\n",
    "            false_x_rate = (preds_squeezed_un[quiet_mask] >= x_class_threshold).float().mean()\n",
    "            self.log(f\"{mode}_false_m_rate\", false_m_rate, sync_dist=True)\n",
    "            self.log(f\"{mode}_false_x_rate\", false_x_rate, sync_dist=True)\n",
    "\n",
    "        # Calculate standard metrics (on normalized values)\n",
    "        mae = F.l1_loss(preds_squeezed, sxr)\n",
    "        mse = F.mse_loss(preds_squeezed, sxr)\n",
    "\n",
    "        # Log all metrics\n",
    "        self.log(f\"{mode}_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.log(f\"{mode}_mae\", mae, sync_dist=True)\n",
    "        self.log(f\"{mode}_mse\", mse, sync_dist=True)\n",
    "        self.log(f\"{mode}_avg_weight\", total_weights.mean(), sync_dist=True)\n",
    "        self.log(f\"{mode}_max_weight\", total_weights.max(), sync_dist=True)\n",
    "        self.log(f\"{mode}_mx_flare_ratio\", (sxr_un >= m_class_threshold).float().mean(), sync_dist=True)  # FIXED: Use unnormalized\n",
    "\n",
    "        # FIXED: Log current learning rate from optimizer\n",
    "        if mode == \"train\":\n",
    "            current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "            self.log('learning_rate', current_lr, on_step=True, on_epoch=True, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")\n",
    "\n",
    "    # Optional: Add learning rate warmup\n",
    "    def on_train_epoch_start(self):\n",
    "        # Warmup for first 5 epochs\n",
    "        if self.current_epoch < 5:\n",
    "            warmup_lr = self.lr * (self.current_epoch + 1) / 5\n",
    "            for param_group in self.trainer.optimizers[0].param_groups:\n",
    "                param_group['lr'] = warmup_lr\n",
    "\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_channels,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            patch_size,\n",
    "            num_patches,\n",
    "            dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels: Number of channels of the input (3 for RGB)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers: Number of layers to use in the Transformer\n",
    "            num_classes: Number of classes to predict\n",
    "            patch_size: Number of pixels that the patches have per dimension\n",
    "            num_patches: Maximum number of patches an image can have\n",
    "            dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels * (patch_size ** 2), embed_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, num_classes))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:, : T + 1]\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)  # [T+1, B, embed_dim]\n",
    "\n",
    "        attention_weights = []\n",
    "        for block in self.transformer_blocks:\n",
    "            if return_attention:\n",
    "                x, attn_weights = block(x, return_attention=True)\n",
    "                attention_weights.append(attn_weights)\n",
    "            else:\n",
    "                x = block(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]  # Take CLS token\n",
    "        out = self.mlp_head(cls)\n",
    "\n",
    "        if return_attention:\n",
    "            return out, attention_weights\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"Attention Block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of input and attention feature vectors\n",
    "            hidden_dim: Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            dropout: Amount of dropout to apply in the feed-forward network\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "\n",
    "        if return_attention:\n",
    "            attn_output, attn_weights = self.attn(inp_x, inp_x, inp_x, average_attn_weights=False)\n",
    "            x = x + attn_output\n",
    "            x = x + self.linear(self.layer_norm_2(x))\n",
    "            return x, attn_weights\n",
    "        else:\n",
    "            attn_output = self.attn(inp_x, inp_x, inp_x)[0]\n",
    "            x = x + attn_output\n",
    "            x = x + self.linear(self.layer_norm_2(x))\n",
    "            return x\n",
    "\n",
    "\n",
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size: Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels: If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)  # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1, 2)  # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4)  # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ],
   "id": "45c1d861e4f82610",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T13:55:35.705911Z",
     "start_time": "2025-07-25T13:55:34.632202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#from flaring.forecasting.models.vision_transformer_custom import ViT\n",
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/flaring/forecasting/training/MEGS-AI ViT Testing Griffin/hztj5skd/checkpoints/epoch=87-step=99000.ckpt\", weights_only=False)\n",
    "\n",
    "# See what's actually in the checkpoint\n",
    "print(\"Keys in checkpoint:\", checkpoint.keys())\n",
    "\n",
    "# Lightning will automatically load the hyperparameters\n",
    "model = ViT.load_from_checkpoint(\n",
    "    \"/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/flaring/forecasting/training/MEGS-AI ViT Testing Griffin/hztj5skd/checkpoints/epoch=87-step=99000.ckpt\"\n",
    ")\n",
    "model.eval()"
   ],
   "id": "97de48dff15f9de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in checkpoint: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (model): VisionTransformer(\n",
       "    (input_layer): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-5): 6 x AttentionBlock(\n",
       "        (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mlp_head): Sequential(\n",
       "      (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with torch.no_grad():\n",
    "    # Your prediction code here\n",
    "    predictions = model(input_data)"
   ],
   "id": "c83a46eee97cf729"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
