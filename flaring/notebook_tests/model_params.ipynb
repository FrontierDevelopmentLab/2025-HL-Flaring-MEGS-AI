{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:19.046608Z",
     "start_time": "2025-08-04T17:18:19.025422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.utilities.model_summary import LayerSummary, ModelSummary\n",
    "from torch.nn import HuberLoss\n",
    "import sys\n",
    "sys.path.append(\"/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/\") # go to parent dir\n",
    "print(sys.path)\n",
    "from flaring.forecasting.models.base_model import BaseModel\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class LinearIrradianceModel(BaseModel):\n",
    "    def __init__(self, d_input, d_output, loss_func=HuberLoss(), lr=1e-4):\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        model = nn.Linear(2 * self.n_channels, self.outSize)\n",
    "        super().__init__(model=model, loss_func=loss_func, lr=lr)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "\n",
    "\n",
    "        # Debug: Print input shape\n",
    "        #print(f\"Input shape to LinearIrradianceModel.forward: {x.shape}\")\n",
    "\n",
    "        # Expect x shape: (batch_size, H, W, C)\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D input tensor (batch_size, H, W, C), got shape {x.shape}\")\n",
    "        if x.shape[-1] != self.n_channels:\n",
    "            raise ValueError(f\"AIA image has {x.shape[-1]} channels, expected {self.n_channels}\")\n",
    "\n",
    "        # Calculate mean and std across spatial dimensions (H,W)\n",
    "        # First permute to (batch_size, C, H, W)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        # Now calculate mean/std across dimensions 2 and 3 (H,W)\n",
    "        mean_irradiance = torch.mean(x, dim=(2, 3))  # Shape: (batch_size, n_channels)\n",
    "        std_irradiance = torch.std(x, dim=(2, 3))    # Shape: (batch_size, n_channels)\n",
    "\n",
    "        # Debug: Print shapes after mean and std\n",
    "        #print(f\"mean_irradiance shape: {mean_irradiance.shape}, std_irradiance shape: {std_irradiance.shape}\")\n",
    "\n",
    "        input_features = torch.cat((mean_irradiance, std_irradiance), dim=1)  # Shape: (batch_size, 2 * n_channels)\n",
    "        #print(f\"Input features shape to linear layer: {input_features.shape}\")\n",
    "\n",
    "        if input_features.shape[1] != 2 * self.n_channels:\n",
    "            raise ValueError(f\"Expected {2 * self.n_channels} features, got {input_features.shape[1]}\")\n",
    "\n",
    "        return self.model(input_features)\n",
    "\n",
    "class HybridIrradianceModel(BaseModel):\n",
    "    def __init__(self, d_input, d_output, cnn_model='resnet', ln_model=True, ln_params=None, lr=1e-4, cnn_dp=0.75, loss_func=HuberLoss()):\n",
    "        super().__init__(model=None, loss_func=loss_func, lr=lr)\n",
    "        self.n_channels = d_input\n",
    "        self.outSize = d_output\n",
    "        self.ln_params = ln_params\n",
    "        self.ln_model = None\n",
    "        if ln_model:\n",
    "            self.ln_model = LinearIrradianceModel(d_input, d_output, loss_func=loss_func, lr=lr)\n",
    "        if self.ln_params is not None and self.ln_model is not None:\n",
    "            self.ln_model.model.weight = nn.Parameter(self.ln_params['weight'])\n",
    "            self.ln_model.model.bias = nn.Parameter(self.ln_params['bias'])\n",
    "        self.cnn_model = None\n",
    "        self.cnn_lambda = 1.\n",
    "        if cnn_model == 'resnet':\n",
    "            #deeper model\n",
    "            self.cnn_model = nn.Sequential(\n",
    "\n",
    "                nn.Conv2d(d_input, 64, kernel_size=7, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),  # Add batch normalization\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 64, kernel_size=7, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                nn.AdaptiveAvgPool2d((2, 2)),\n",
    "                nn.Linear(2048, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(cnn_dp),\n",
    "                nn.Linear(128, d_output),\n",
    "            )\n",
    "\n",
    "        elif cnn_model.startswith('efficientnet'):\n",
    "            raise NotImplementedError(\"EfficientNet requires timm; replace with custom CNN or install timm\")\n",
    "        if self.ln_model is None and self.cnn_model is None:\n",
    "            raise ValueError('Please pass at least one model.')\n",
    "\n",
    "    def forward(self, x, sxr=None, **kwargs):\n",
    "        # If x is a tuple (aia_img, sxr_val), extract the AIA image tensor\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[0]\n",
    "\n",
    "        # Expect x shape: (batch_size, H, W, C)\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(f\"Expected 4D input tensor (batch_size, H, W, C), got shape {x.shape}\")\n",
    "        if x.shape[-1] != self.n_channels:\n",
    "            raise ValueError(f\"AIA image has {x.shape[-1]} channels, expected {self.n_channels}\")\n",
    "\n",
    "        # Convert to (batch_size, C, H, W) for CNN\n",
    "        x_cnn = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.ln_model is not None and self.cnn_model is not None:\n",
    "            # For linear model, keep original (B,H,W,C) format\n",
    "            return self.ln_model(x) + self.cnn_lambda * self.cnn_model(x_cnn)\n",
    "        elif self.ln_model is not None:\n",
    "            return self.ln_model(x)\n",
    "        elif self.cnn_model is not None:\n",
    "            return self.cnn_model(x_cnn)\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def set_train_mode(self, mode):\n",
    "        if mode == 'linear':\n",
    "            self.cnn_lambda = 0\n",
    "            if self.cnn_model: self.cnn_model.eval()\n",
    "            if self.ln_model: self.ln_model.train()\n",
    "        elif mode == 'cnn':\n",
    "            self.cnn_lambda = 0.01\n",
    "            if self.cnn_model: self.cnn_model.train()\n",
    "            if self.ln_model: self.ln_model.eval()\n",
    "        elif mode == 'both':\n",
    "            self.cnn_lambda = 0.01\n",
    "            if self.cnn_model: self.cnn_model.train()\n",
    "            if self.ln_model: self.ln_model.train()\n",
    "        else:\n",
    "            raise NotImplementedError(f'Mode not supported: {mode}')"
   ],
   "id": "5e59724efe1f0f0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/jupyter_debug', '/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev', '/tmp/w4scFYbshp', '/home/griffingoodwin/.pycharm_helpers/pydev', '/home/griffingoodwin/.pycharm_helpers/jupyter_debug', '/opt/conda/envs/Flare_detection/lib/python310.zip', '/opt/conda/envs/Flare_detection/lib/python3.10', '/opt/conda/envs/Flare_detection/lib/python3.10/lib-dynload', '', '/home/griffingoodwin/.local/lib/python3.10/site-packages', '/opt/conda/envs/Flare_detection/lib/python3.10/site-packages', '/tmp/tmpotxgyzav', '/opt/conda/envs/Flare_detection/lib/python3.10/site-packages/setuptools/_vendor', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/', '/home/griffingoodwin/2025-HL-Flaring-MEGS-AI/']\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:19.806915Z",
     "start_time": "2025-08-04T17:18:19.803626Z"
    }
   },
   "cell_type": "code",
   "source": "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
   "id": "3cf5d6f0882bef27",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:20.276236Z",
     "start_time": "2025-08-04T17:18:20.179264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Model = HybridIrradianceModel(6,1,cnn_dp=.2)\n",
    "\n"
   ],
   "id": "ba75ebc2b0eae142",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:20.572178Z",
     "start_time": "2025-08-04T17:18:20.566990Z"
    }
   },
   "cell_type": "code",
   "source": "Model",
   "id": "d5cc05d47a9994b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridIrradianceModel(\n",
       "  (loss_func): HuberLoss()\n",
       "  (ln_model): LinearIrradianceModel(\n",
       "    (model): Linear(in_features=12, out_features=1, bias=True)\n",
       "    (loss_func): HuberLoss()\n",
       "  )\n",
       "  (cnn_model): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU()\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "    (29): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (30): ReLU()\n",
       "    (31): Dropout(p=0.2, inplace=False)\n",
       "    (32): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (33): ReLU()\n",
       "    (34): Dropout(p=0.2, inplace=False)\n",
       "    (35): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (36): ReLU()\n",
       "    (37): Dropout(p=0.2, inplace=False)\n",
       "    (38): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (39): ReLU()\n",
       "    (40): Dropout(p=0.2, inplace=False)\n",
       "    (41): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (42): ReLU()\n",
       "    (43): Dropout(p=0.2, inplace=False)\n",
       "    (44): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T17:18:21.231807Z",
     "start_time": "2025-08-04T17:18:21.225808Z"
    }
   },
   "cell_type": "code",
   "source": "ModelSummary(Model)",
   "id": "501891fbdce3486c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name      | Type                  | Params | Mode \n",
       "------------------------------------------------------------\n",
       "0 | loss_func | HuberLoss             | 0      | train\n",
       "1 | ln_model  | LinearIrradianceModel | 13     | train\n",
       "2 | cnn_model | Sequential            | 12.2 M | train\n",
       "------------------------------------------------------------\n",
       "12.2 M    Trainable params\n",
       "0         Non-trainable params\n",
       "12.2 M    Total params\n",
       "48.988    Total estimated model params size (MB)\n",
       "49        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T16:45:41.759288Z",
     "start_time": "2025-08-04T16:45:41.755632Z"
    }
   },
   "cell_type": "code",
   "source": "import flaring.forecasting.models.vision_transformer_custom as vit_custom",
   "id": "88614e4b77d6cc42",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:29:44.317805Z",
     "start_time": "2025-08-04T15:29:44.313791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kwarg = {\n",
    "    'embed_dim': 512,\n",
    "    'num_channels': 6,\n",
    "    'num_classes': 1,\n",
    "    'patch_size': 16,\n",
    "    'num_patches': 1024,\n",
    "    'hidden_dim': 1024,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 0.0001}"
   ],
   "id": "b4e650ec7ec7727b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:30:12.462378Z",
     "start_time": "2025-08-04T15:30:12.324639Z"
    }
   },
   "cell_type": "code",
   "source": "v = vit_custom.ViT(kwarg, sxr_norm=None)",
   "id": "b3e09333e878cb5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:30:17.609063Z",
     "start_time": "2025-08-04T15:30:17.601677Z"
    }
   },
   "cell_type": "code",
   "source": "ModelSummary(v)",
   "id": "d0cf71415eb0b087",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name  | Type              | Params | Mode \n",
       "----------------------------------------------------\n",
       "0 | model | VisionTransformer | 13.9 M | train\n",
       "----------------------------------------------------\n",
       "13.9 M    Trainable params\n",
       "0         Non-trainable params\n",
       "13.9 M    Total params\n",
       "55.722    Total estimated model params size (MB)\n",
       "73        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "540637c7a50f0ddf"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
